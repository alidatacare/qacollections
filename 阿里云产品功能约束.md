阿里云产品功能约束

maxCompute
Q:有类似ptkill之类的方法批量kill超时任务的方式吗？
A:目前没有批量kill的操作，只能一个个kill instanceid 操作


Q：下面关于DataWorks平台上数据管理—>查找数据—>血缘信息相关的问题，需要解答一下：
1.血缘信息的上下游表的相关信息，多久会更新？

2.在查一些表的上游表时，会发现有很多重复的表名；在查一些下游表时，一些临时的表名也在下游表名里出现, 而且下游表同名的情况出现多次。还有就是表的下游表表名和它本身重名。是什么原因？

3.关于表的上下游表是从我们每天执行的任务里面提取出来的相关关系吗？

A:相关回答如下：

每天更新一次，更新方式有两种:
实时更新，建表之后立刻能看到。
实时更新失败会有每天全量更新，一般在早上7点之前能够跑完。
任务是根据定时任务来推测的。
是的，是通过调度系统运行的任务。

Q:ODPS中可以设置表的过期时间，是否有办法设置分区的过期时间？
A:暂不支持。

Q：MaxCompute是否支持restful接口？
A：目前还不支持纯restful api，仅支持python和java sdk的。

Q:新建子管理账号，但子账号不能访问MaxCompute的功能，是什么原因？
A:这个是权限问题，需要主账户登录将您的子账户加入数加控制台。


Q：MaxCompute 项目中的 Owner 能否更换为子账号？

A：项目的 Owner 不可以更换，谁创建的 Project，谁就是 Owner。您可以将 Admin 的角色赋予子账号。

Q：与 Owner 相比，Admin 角色有哪些限制？

A： 与 Owner 相比，Admin 角色不能进行如下操作：

Admin 角色不能将 admin 权限指派给用户。

不能设定项目空间的安全配置。

不能修改项目空间的鉴权模型。

Admin 角色所对应的权限不能被修改。



Q:使用DataWorks提交任务时，设置${bdp.system.bizdate}这个时间参数,如果想取一年前、一个月前、半年前和一周前的时间应分别怎样设置？
A:请参考参数配置，目前是无法基于bdp.system.bizdate进行自定义的。


Q：在 cmd_file 中，是一系列 SQL 和 MapReduce 程序，cmd_file 其实相当于一个脚本程序，为了复用该脚本程序，里面能可以用$变量吗？可以在调用时传入参数吗？

A：目前不支持传入变量。有个变通的方式：在 cmd.sh 脚本文件中，动态构造 MaxCompute 的执行语句。



使用 MaxCompute 客户端连接服务的时候，报错如下：

试用
 FAILED: ODPS-0410031:Authentication request expired - the expire time interval exceeds the max limitation: 900000, max_interval_date:900000,expire_date:2015-12-23T10:15:31.000Z,now_date:2015-12-23T02:16:00.000Z


 问题原因
出现上述报错，是因为安装客户端的机器本地时间和 MaxCompute 服务器上的时间不一样，相差超过 15 分钟，导致被服务器认为请求超时而拒绝服务。

解决方法
可以将机器的本地时间调整后，重新打开客户端。国内的region，机器本地时间获取当前时间既可。


Q:MaxCompute支持快照吗？changelog的设置方式是什么？
A:不支持快照，也没有changelog之类的配置功能。


用户需求
需要读取不同项目中的数据。

解决方法
目前已经支持跨项目的数据读写。您可以进行以下操作：

准备一个账号，同时拥有两个项目表里的权限。可根据 授权 来对权限进行设置。

设置好权限后，可以使用 项目名.表名 来访问表，如下所示：

试用
INSERT OVERWRITE TABLE Prj1.tab1 SELECT * FROM Prj1.Tab1 t1 JOIN Prj2.Tab2 t2 ON t1.Col1 = t2.Col2；

MaxCompute之Tunnel命令相关问题

Q：是否支持 ascii 字符的分隔符？
A：命令行方式不支持，配置文件可以用十六进制表示。如 \u000A，表示回车。

Q：文件大小是否有限制？
A：文件大小没有限制，但一次 upload 无法超过 24 小时，可以根据实际上传速度和时间来估算能够上传的数据量。

Q：记录大小是否有限制？
A：记录大小不能超过 200M。

Q：是否要使用压缩？
A：默认会使用压缩，如果带宽允许的情况下，可以关掉压缩。

Q：同一个表或 partition 是否可以并行上传？
A：可以。

Q：是否支持不同字符编码？
A：支持不同的编码格式参数，带 bom 标识文件不需要指定编码。

Q：导入后的脏数据怎么处理？
A：导入结束后，如果有脏数据可以通过 tunnel show bad [sessionid] 查看脏数据。

Q：上传下载的文件路径是否可以有空格？
A：可以有空格，参数需要用双引号括起来。

Q：为什么会出现乱码？
A：可能是上传文件的字符编码和工具指定的编码不符。

Q：导入数据最后一列为什么多出\r符号？
A：windows 的换行符是\r\n，macosx 和 linux 的换行符是\n，tunnel 命令使用系统换行符作为默认列分隔符，所以从 macosx 或 linux 上传 windows 编辑保存的文件会把\r作为数据内容导进去。

Q：Tunnel 下载/上传速度正常速度范围是多少？
A：Tunnel 下载上传，受网络因素影响较大，正常网络情况下速度范围在 1MB/s-20MB/s 区间内。

Q:odpscmd tunnel是否支持.dbf后缀非加密数据库文件?
A:Tunnel支持文本文件，不支持二进制的文件。


用户需求
如何用Tunnel上传数据的时候实现类似Overwrite的追加功能。

解决方案
目前Tunnel只提供Append的插入方式，如果用户需要Overwrite，可以考虑先删除分区里的数据后再插入。

关于删除的方法，可以参考这里：

如果表是分区表，可以使用ALTER TABLE TABLE_NAME DROP [IF EXISTS] PARTITION partition_spec;

如果是非分区表，可以用TRUNCATE TABLE table_name;


Q：Datahub 和 Tunnel 应用场景的区别是什么？
A：Datahub 用于实时上传数据的场景，主要用于流式计算的场景。数据上传后会保存到实时表里，后续会在几分钟内通过定时任务的形式同步到离线表里，供离线计算使用。

Tunnel 用于批量上传数据到离线表里，适用于离线计算的场景。


MapReduce
Q：MapReduce 的输入源可以是视图吗？
A：不可以，只能是表。

Q：MapReduce 中是否可以调用 shell 文件？
A：不能，会被沙箱阻挡。

Q：MapReduce 的结果写入到表或分区时，会覆盖还是追加数据？
A：会覆盖掉原有的表数据或者分区数据。



问题症状
您在使用MaxCompute Java SDK的Tunnel传输数据的时候，有时会发现数据传输等待的时间很长，语句的执行性能不好。遇到这种情况，可能是因为您的MaxCompute小文件过多，从而影响性能。

问题原因
小文件产生的场景有很多，请参考下文：更多信息。

解决方案
您可以通过以下的命令来查看表中的小文件数量：

试用
desc extended + 表名
您尝试执行下面的SQL语句来整合小文件：

试用
  set odps.merge.cross.paths=true;
  set odps.merge.max.partition.count=100;   --默认优化10个分区，此时设置为优化100个分区。
  ALTER TABLE 表名[partition] MERGE SMALLFILES;




DataWorks
DataWorks安全中心
Q：通过安全中心可以申请什么权限？
A：您可通过安全中心页面申请DataWorks工作空间内的表权限，包括开发环境和生产环境。

Q：为什么在申请时，有时可以选择字段，有时不可以选择？
A：如果该工作空间开启了LabelSecurity，即可在申请时选择字段，未开启则只能整表申请。

Q：提交申请后，需要谁进行审批？
A：提交的申请申请需要项目管理员或表Owner进行审批，其中任何一个审批通过/拒绝，则审批完成。

Q：为什么我提交了一个申请，在我的申请中却看到两个申请单？
A：因为您的申请单中包含的数据表的表Owner不同，安全中心会按照表Owner对于申请单自动进行拆分。

Q：为什么有的字段只申请1个月权限，审批完成后查看变为永久？
A：说明字段的安全等级为0或者小于等于您账号的安全等级。

Q：为什么有的表和字段没有申请权限，但能看到有权限？
A：出现此情况有以下两种可能：
除安全中心外，管理员还可通过控制台命令行给您授权。
如果您是通过安全中心进行了申请，则说明字段的安全等级为0或者小于等于您账号的安全等级。
Q：为什么我并没有审批某个待我审批中的申请单，却没有了？
A：因为申请单由项目管理员或表Owner进行审批，其他项目管理员或表Owner在您之前完成了审批，因此该申请单已成为完结状态，便从您的待我审批中消失了。

Q：查询某个工作空间和环境，提示MaxCompute项目异常，无法进行操作，该如何处理？
A：请将提示框及框内的错误编码发给项目管理员，由他进行问题的排查及解决。

Q：为什么我交还/回收某个字段权限却无效？
A：只能交还/回收字段的安全等级大于账号安全等级的字段，对于安全等级为0或者小于等于账号安全等级的字段，无法进行字段权限的交还/回收。


DataWorks数据服务
Q：是否必须开通API网关？
A：API网关提供了API托管服务，如果您的API计划对外开放调用，则必须先开通API网关服务。

Q：数据源在哪里配置？
A：数据源需要在DataWorks数据集成数据源中进行配置。配置好后，数据服务会自动读取数据源信息。


Q：数据服务中的API分组是干什么用的，与API网关中的分组有什么关联？
A：API分组一个特定功能或场景的API集合，是数据服务中API的最小组织单元，对应于API网关中的分组概念。简单来说，二者是等同的。数据服务中的API发布到API网关时，系统会在API网关中自动创建一个同名的分组。

Q：如何设置API分组比较合理？
A：一般将解决同一个问题或者相似功能的API放在一个分组当中。例如根据城市名称查看天气API与根据经纬度查询天气API这两个就可以放在一个名为天气查询的API分组中。

Q：最多可以创建多少个API分组？
A：目前一个云账号下最多可创建100个API分组。

Q：什么情况下要开启API返回结果分页功能？
A：默认情况下，API最多只会返回500条查询结果。因此当API返回结果有可能超过500条时，请开启返回结果分页功能。当API无请求参数时，一般返回结果会比较多，系统会强制开启返回结果分页。

Q：生成API是否支持POST请求？
A：生成API当时仅支持GET请求。

Q：是否支持HTTPS协议？
A：当前尚不支持HTTPS，在后续的版本迭代中有可能会支持HTTPS，敬请期待。



DataHub

限制描述
限制项	                 描述	                                                值域范围
活跃shard数	         每个topic中活跃shard数量限制	                                     (0,10]  
                                                                      （公测限制，流量超出10个Shard
                                                                        承载能力请联系管理员提升Quota）
总shard数	         每个topic中总shard数量限制	                                     (0,512]
Http BodySize	     http请求中body大小限制	                                      4MB
单个String长度	     数据中单个String字段长度限制	                                   1MB
Merge/Split频率限制	 每个新产生的shard在一定时间内不允许进行Merge/Split操作	           5s
QPS限制	             每个Shard写入QPS限制(非Record/s，Batch写入同一Shard仅计算为1次)	   1000
Throughput限制	     每个Shard写入每秒吞吐限制	                                        1MB
Project限制	         每个云账号能够创建的Project上限	                                     5
Topic限制	         每个Project内能创建的Topic数量限制，如有特殊请求请联系管理员	         20
Topic Lifecycle限制	 每个Topic中数据保存的最大时长，单位是天	                            [1,7]

Project	项目名称	[3,32]	英文字母开头，仅允许英文字母、数字及“_”，大小写不敏感。
Topic	主题名称	[1,128]	英文字母开头，仅允许英文字母、数字及“_”，大小写不敏感。


DataHub在RAM的访问控制中的资源体系包含Project、Topic和Subscription。目前支持Project、Topic和Subscription级别的鉴权，并不支持Shard的访问控制。其中Subscription是指对某个特定Project下的Topic的一次订阅。



